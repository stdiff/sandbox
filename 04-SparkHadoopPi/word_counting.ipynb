{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word Counting with Hadoop / Spark\n",
    "\n",
    "The aim of this document is to show and compare several ways of counting word with MapReduce.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "Following [this blog entry](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/), we use the following text data (without changing the names of files).\n",
    "\n",
    "- [The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson](http://www.gutenberg.org/etext/20417)\n",
    "- [The Notebooks of Leonardo Da Vinci](http://www.gutenberg.org/etext/5000)\n",
    "- [Ulysses by James Joyce](http://www.gutenberg.org/ebooks/4300)\n",
    "\n",
    "First of all we put these text file on HDFS. We assume that there are *only the three text files in the current directry*. Then we can put the text files by the following two commands\n",
    "\n",
    "    $ hadoop fs -mkdir word_count\n",
    "$ hadoop fs -put * word_count\n",
    "\n",
    "We may check by the following command if the files are in the directry `word_count`.\n",
    "\n",
    "    $ hadoop fs -ls -R word_count\n",
    "    -rw-r--r--   1 crescent supergroup     674570 2017-02-26 21:23 word_count/20417.txt.utf-8\n",
    "    -rw-r--r--   1 crescent supergroup    1580927 2017-02-26 21:23 word_count/4300-0.txt\n",
    "    -rw-r--r--   1 crescent supergroup    1428841 2017-02-26 21:23 word_count/5000-8.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remarks\n",
    "\n",
    "- For the terminology and idea for MapReduce see [this tutorial](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm).\n",
    "- To illustrate the structure of the code, the implementation is as small as we can. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hadoop Streaming\n",
    "\n",
    "Hadoop Streaming allows us to make use of MapReduce in the most intuitive way in any programming languages. We only need two executable scripts/binaries: `mapper` and `reducer`. These correspond \"Map\" and \"Reduce\" in MapReduce respectively. \n",
    "\n",
    "- Unfortunately a script in [Python3 is not working](http://serverfault.com/q/807839).\n",
    "- The following codes are written in Perl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/perl\n",
      "\n",
      "use strict;\n",
      "use warnings;\n",
      "\n",
      "while (my $line = <STDIN>) {\n",
      "  chomp($line);\n",
      "  my @words = split /\\s+/, $line;\n",
      "  foreach my $word (@words) {\n",
      "    print $word . \"\\t1\\n\";\n",
      "  }\n",
      "}\n",
      "\n",
      "exit;\n"
     ]
    }
   ],
   "source": [
    "with open('mapper.pl','r') as fo:\n",
    "    for line in fo:\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/perl\n",
      "\n",
      "use strict;\n",
      "use warnings;\n",
      "\n",
      "my $current_word = undef;\n",
      "my $current_count = 0;\n",
      "\n",
      "while (my $line = <STDIN>) {\n",
      "  my ($word, $count) = split /\\s+/, $line;\n",
      "\n",
      "  if ($word ne $current_word) {\n",
      "    emit_pair($current_word,$current_count);\n",
      "    $current_word = $word;\n",
      "    $current_count = 0;\n",
      "  }\n",
      "\n",
      "  $current_count += $count;\n",
      "}\n",
      "\n",
      "emit_pair($current_word,$current_count);\n",
      "\n",
      "exit;\n",
      "\n",
      "sub emit_pair{\n",
      "  print \"$_[0]\\t$_[1]\\n\";\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('reducer.pl','r') as fo:\n",
    "    for line in fo:\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "We can check with the following command if the scripts work.\n",
    "\n",
    "    $ cat data/* | ./mapper.pl | sort -k1,1 | ./reducer.pl\n",
    "\n",
    "The following command starts the MapReduce procedure.\n",
    "\n",
    "    $ hadoop jar /opt/hadoop-2.7.3/share/hadoop/tools/lib/hadoop-streaming-2.7.3.jar \\\n",
    "    > -mapper mapper.pl -reducer reducer.pl \\\n",
    "    > -input word_count -output word_count_output \\\n",
    "    > -file mapper.pl -file reducer.pl\n",
    "\n",
    "The output directory `word_count_ouptput` must not exsist. If it exists, we must delete it or use a different directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## mrjob\n",
    "\n",
    "`mrjob` is a [Python library for MapReduce](https://pythonhosted.org/mrjob/).\n",
    "\n",
    "- We can write multi-steps job in a single script.\n",
    "- We can execute the script for testing without MapReduce.\n",
    "- We do not have to put the input files on HDFS in advance. We get the result on the stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python3\n",
      "\n",
      "from mrjob.job import MRJob\n",
      "\n",
      "class WordCount(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        for word in line.rstrip().split():\n",
      "            yield (word, 1)\n",
      "\n",
      "    def combiner(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "     WordCount.run()\n"
     ]
    }
   ],
   "source": [
    "with open('with_mrjob.py','r') as fo:\n",
    "    for line in fo:\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "Executing the following command, we can check the code without MapReduce:\n",
    "\n",
    "    ./with_mrjob.py data/* > count_without_mr\n",
    "\n",
    "We can execute the code with with MapReduce by the following command.\n",
    "\n",
    "    $ ./with_mrjob.py -r hadoop hdfs:///user/crescent/word_count/* > count_with_mr\n",
    "\n",
    "We can specify local files instead of files on HDFS.\n",
    "\n",
    "Note that the results might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23973c23973\n",
      "< \"The\"\t3524\n",
      "---\n",
      "> \"The\"\t3523\n",
      "27938a27939,27940\n",
      "> \"\\ufeff\"\t1\n",
      "> \"\\ufeffThe\"\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "diff = Popen([\"diff\",\"count_with_mr\",\"count_without_mr\"],stdout=PIPE).communicate()[0]\n",
    "print(diff.decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Spark\n",
    "\n",
    "Spark can execute a MapReduce procedure more efficiently than Hadoop. Moreover the Spark APIs are very simple and convenient to write a MapReduce task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext() # this takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdfs://localhost:9000/user/crescent/word_count MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files = sc.textFile('hdfs://localhost:9000/user/crescent/word_count')\n",
    "#text_files = sc.textFile('data') ## for local files\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of The Outline of Science, Vol. 1 (of 4), by ',\n",
       " 'J. Arthur Thomson',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: The Outline of Science, Vol. 1 (of 4)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files.take(10) ## first 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## write a MR procedure\n",
    "word_count = text_files.flatMap(lambda line: [(w,1) for w in line.rstrip().split()])\\\n",
    "                       .reduceByKey(lambda a,b: a+b)\n",
    "word_count ## this does not start the MR procedure. (lazy evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(Multifarnham.', 1),\n",
       " ('_non', 1),\n",
       " ('Lucifer,', 1),\n",
       " ('divers', 17),\n",
       " ('Wanted,', 1),\n",
       " ('528:', 1),\n",
       " ('bob.', 6),\n",
       " ('(black', 3),\n",
       " ('vuole,', 1),\n",
       " ('Ladies’', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.take(10) ## starts the MapReduce procedure. It finishes within one minute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
